%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% From a template maintained at https://github.com/jamesrobertlloyd/cbl-tikz-poster
%
% Code near the top should be fairly standard and not need to be changed
%  - except for the document class
% Code lower down is more likely to be customised
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[landscape,a0b,final,a4resizeable]{include/a0poster}

\usepackage{multicol}
\usepackage{color}
\usepackage{morefloats}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}


\usepackage{include/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{blue}{rgb}{0,0,1}

\usepackage{dsfont}

\input{include/jlposter.tex}

\input{include/preamble.sty}
\newcommand{\vv}{\mathbf{v}}

\begin{document}
\begin{poster} 

% Potentially add some space at the top of the poster
\vspace{0\baselineskip}


%%% Header
\begin{center}
\begin{pcolumn}{0.99}

\newcommand{\logowidth}{0.099\textwidth}

\pbox{0.99\textwidth}{}{linewidth=2mm,framearc=0.3,linecolor=camdarkblue,fillstyle=gradient,gradangle=0,gradbegin=white,gradend=white,gradmidpoint=1.0,framesep=1em}{
%
%%% Cambridge Logo
\begin{minipage}[c]{\logowidth}
  \begin{center}
    \includegraphics[width=18cm]{badges/hips-logo.png}
  \end{center}
\end{minipage}
%
%%% Title
\begin{minipage}[c][9cm][c]{0.76\textwidth}
  \begin{center}
    {\sffamily \VeryHuge \textbf{Gradient-based Hyperparameter Optimization}}\\[10mm]
    {\huge\sffamily \Huge Dougal Maclaurin, David Duvenaud, Ryan P. Adams\\[7.5mm]
    %\texttt{\{ti242, dkd23, zoubin\}@cam.ac.uk}
    }
  \end{center}
\end{minipage}
%
%
% Harvard logo
\begin{minipage}[c]{\logowidth}
  \begin{flushright}
    \includegraphics[width=8cm,trim=2em 0em 2em 2em, clip]{badges/harvard}
  \end{flushright}
\end{minipage}
%
}
\end{pcolumn}
\end{center}

\vspace*{1.5cm}

\large


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{multicols}{3}

\mysection{Motivation}

%\vspace{0.5in}

%\begin{minipage}[c]{0.17\columnwidth}
\begin{itemize}
  \item Hyperparameters are everywhere
  \item Gradient-free optimization fails in high dimensions
  \item Why not use gradients? Then we could optimize thousands of hyperparameters!
\end{itemize}

\vspace{0.5in}

\mysection{Stochastic gradient descent is a function}

\begin{tabular}{cc}
\begin{minipage}[c]{0.5\columnwidth}
\begin{itemize}
  \item We want to optimize validation loss
  \item Validation loss is a function of SGD
  \item SGD is a smooth function mapping \\(init weights, hypers) $\rightarrow$ trained weights
  \item Let's compute its gradients!
\end{itemize}
\end{minipage} & 
\begin{minipage}[c]{0.5\columnwidth}
\includegraphics[width=\columnwidth]{../talks/talkfigs/learning_curves_3.pdf}
\end{minipage}
\end{tabular}


\mysection{Example: Optimizing learning rate schedules}

We can optimize learning rate schedules separately for each layer of a neural network and \emph{each iteration} of training:
\begin{center}
Learning rate

\includegraphics[width=0.7\columnwidth]{../experiments/Feb_3_training_schedules/3_adam_50/schedules_small.pdf}
\end{center}

How did we optimize it?  By SGD on top of SGD, using meta-gradients:

\begin{center}
Hyper-gradient with respect to learning rate

\includegraphics[width=0.7\columnwidth]{../experiments/Feb_3_training_schedules/5_initial_gradient/schedules_small.pdf}
\end{center}


\newpage %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\mysection{Optimizing initialization distributions}


\begin{tabular}{cc}
\begin{minipage}[c]{0.6\columnwidth}
\begin{itemize}
  \item We can optimize thousands of hyperparameters
  \item For instance, detailed weight initialization schemes
  \item Meta-learned values roughly match $1 / \sqrt(N)$ heuristic
\end{itemize}
\end{minipage} & 
\begin{minipage}[c]{0.4\columnwidth}
\begin{center}
Weight initialization scale

\hspace{-1em}\includegraphics[width=0.8\columnwidth]{../experiments/Feb_3_training_schedules/3_adam_50/init_weight_learning_curve.pdf}
\end{center}
\end{minipage}
\end{tabular}


\mysection{Optimizing training data}

\begin{center}
Synthetic MNIST training examples

\includegraphics[width=0.6\columnwidth]{../experiments/Jan_19_optimize_data/9_color_bar/fake_data.pdf}
\end{center}

Training data can be viewed as just another hyperparameter.
We meta-learned MNIST training examples starting from blank pixels, optimizing validation loss.

\vspace{0.5em}

\mysection{Optimizing network architecture}

\begin{itemize}
\item Can optimize thousands of regularization params.
\item Architecture can be controlled through regularization.
\item We let the network choose which layers to share in a multi-task problem.
\end{itemize}

\vspace{1em}

\begin{center}
\begin{tabular}{cc}
\rotatebox{90}{\qquad Rotated \qquad \quad Original} & 
\includegraphics[width=0.5\columnwidth]{../experiments/Feb_4_augmented_omniglot/2_rotated_90/all_alphabets.png}
\end{tabular}
\end{center}

\vspace{0.5em}

\begin{itemize}
\item Network learned to share weights between related tasks.
\item Learned sharing works better than all-or-nothing.
\end{itemize}

\vspace{0.5em}

\newcommand{\omniimagea}[2]{\parbox{4em}{\includegraphics[width=3.4cm]{../experiments/Feb_4_augmented_omniglot/2_rotated_90/minifigs/learned_corr_#1_#2.pdf}}}%
\newcommand{\omniimageb}[1]{\omniimagea{#1}{0} & \omniimagea{#1}{1} & \omniimagea{#1}{2}}%


\begin{center}
\begin{tabular}{c@{\hskip 0.9em}ccc@{\hskip 0.9em}c@{\hskip 0.9em}c}%
\renewcommand{\tabcolsep}{1pt}
& Input   & Middle  & Output & Train & Test\\
& weights & weights & weights & error & error \\
\parbox{3.7em}{Separate networks} & \omniimageb{no_sharing}      & 0.61 & 1.34\\ \hline
\parbox{3.7em}{Tied weights}      & \omniimageb{full_sharing}    & 0.90 & 1.25\\ \hline
\parbox{3.7em}{Learned sharing}   & \omniimageb{learned_sharing} & 0.60 & \bf 1.13
\end{tabular}
\end{center}



\newpage %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\mysection{Chaotic learning dynamics}



\begin{tabular}{cc}
\begin{minipage}[c]{0.4\columnwidth}
\begin{itemize}
\item Limitation: Gradient can become chaotic
\item a.k.a.\ exploding gradients
\item Happens when learning rate is near the optimum.
\end{itemize}
\end{minipage} & 
\begin{minipage}[c]{0.55\columnwidth}
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/Jan_14_learning_rate_wiggliness/3/chaos.pdf}
Learning rate
\end{center}
\end{minipage}
\end{tabular}

\vspace{0.5em}

\mysection{Stochastic gradient descent is reversible}

To save memory during reverse-mode differentiation, we run SGD in reverse.

\begin{tabular}{cc}
\begin{minipage}[c]{0.4\columnwidth}
Forward update rule:
{\begin{align*}
\vx_{t+1} & \leftarrow \vx_t + \alpha \vv_t \\
\vv_{t+1} & \leftarrow \beta \vv_t - \nabla L\left(\vx_{t+1}\right)
\end{align*}}
\end{minipage}
\begin{minipage}[c]{0.55\columnwidth}
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/Jan_25_Figure_1/4_naive_reverse/learning_curve_forward.pdf}
\end{center}
\end{minipage}
\end{tabular}


\begin{tabular}{cc}
\begin{minipage}[c]{0.4\columnwidth}
Reverse update rule:
{\begin{align*}
\vv_{t} & \leftarrow \left(\vv_{t+1} + \nabla L\left(\vx_{t+1}\right)\right)/\beta \\
\vx_{t} & \leftarrow \vx_{t+1} - \alpha \vv_t
\end{align*}}
\end{minipage}
\begin{minipage}[c]{0.55\columnwidth}
\begin{center}
\includegraphics[width=\columnwidth]{../experiments/Jan_25_Figure_1/4_naive_reverse/learning_curve_reverse.pdf}
\end{center}
\end{minipage}
\end{tabular}

Need to store tiny corrections to each reversal step to ensure exact reversal.

\vspace{0.5em}

\mysection{Conclusion}

\begin{itemize}
\item We can compute gradients of learning procedures...
\item This lets us optimize thousands of hyperparameters!
\item All code for experiments at \url{github.com/HIPS/hypergrad}
\item We also wrote an autodiff package that works on standard Numpy code:\\ \url{github.com/HIPS/autograd}
\end{itemize}



\end{multicols}
\end{poster}

\end{document}

