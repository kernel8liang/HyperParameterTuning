\input{include/header_beamer}
\usepackage{etex}
\usepackage{tabularx}
\usepackage{include/picins}
\usepackage{include/preamble}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{listings}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{caption}


\definecolor{Blue}{rgb}{0.0,0.0,1.0}
\lstloadlanguages{Python}%
\lstset{language=Python,
        frame=none,
        basicstyle=\tiny\ttfamily\bfseries,
        keywordstyle=[1]\color{Blue},
        keywordstyle=[2]\color{Purple},
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{Green},
        keepspaces=true,
        }
\lstset{language=Python} 

\newcommand{\params}{\vx}
\newcommand{\learnrate}{\alpha}
\newcommand{\decay}{\beta}

\setlength{\columnsep}{0.03\textwidth}
\setlength{\columnseprule}{0.0018\textwidth}
\setlength{\parindent}{0.0cm}
\hypersetup{colorlinks=true,citecolor=blue}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\vv}{\mathbf{v}}

\newcommand{\primal}{elementary}
\newcommand{\Primal}{Elementary}
\newcommand{\numhypers}{N}
\newcommand{\hypers}{{\boldsymbol{\theta}}}
\newcommand{\numsteps}{T}
\newcommand{\decays}{{\boldsymbol{\decay}}}
\newcommand{\stepsize}{\alpha}
\newcommand{\stepsizes}{{\boldsymbol{\stepsize}}}
\newcommand{\gradparams}{\nabla_\params L(\params_t, \hypers, t)}

%\title{Hyperparameter optimization: \\ Why not use gradients?}
\title{Gradient-based Hyperparameter Optimization with Reversible Learning}

\author{
\includegraphics[height=0.16\textwidth]{talkfigs/dougal}
\qquad\qquad
\includegraphics[height=0.16\textwidth, trim=20mm 25mm 0mm 25mm, clip]{talkfigs/david2}
\qquad\qquad
\includegraphics[height=0.16\textwidth]{talkfigs/adams}
\\
Dougal Maclaurin, David Duvenaud, Ryan Adams}

%\institute{Harvard University}
\institute{\includegraphics[height=0.16\textwidth]{talkfigs/harvard.jpg}}


\date{}

\begin{document}

\frame[plain] {\titlepage}

\frame[plain]{
\frametitle{Motivation}
\begin{itemize} 
  \item Hyperparameters are everywhere
  \begin{itemize}
	\item sometimes hidden!
  \end{itemize}
  \item Gradient-free optimization is hard %in high dimensions
  \item Validation loss is a function of hyperparameters
  \item Why not take gradients?
\end{itemize}}

\frame[plain]{\frametitle{Optimizing optimization}
\center ${\params_{final} = \textnormal{SGD} \left(\params_{init}, \textnormal{learn rate}, \textnormal{momentum}, \nabla \textnormal{Loss}(\params, \textnormal{reg}, Data) \right)}$

\includegraphics<1>[width=0.9\columnwidth]{talkfigs/learning_curves_1.pdf}
\includegraphics<2>[width=0.9\columnwidth]{talkfigs/learning_curves_2.pdf}
\includegraphics<3>[width=0.9\columnwidth]{talkfigs/learning_curves_3.pdf}
}



\frame[plain]{\frametitle{A pretty scary function to differentiate}

\vspace{0.3cm}

\center ${J = Loss \left( D_{val}, \textnormal{SGD} \left(\vx_{init}, \learnrate, \decay, \nabla \textnormal{Loss}(D_{train}, \vx, \textnormal{reg}) \right)\right)}$

\vspace{-0.6cm}

\centerline{
\begin{minipage}[t]{0.8\columnwidth}
\begin{algorithm}[H]
\small
   \caption*{Stochastic Gradient Descent}
   \label{alg:sgd}
\begin{algorithmic}[1]
   \State {\bfseries input:} initial $\params_1$, decay $\decays$, learning rate $\stepsizes$, regularization params $\theta$, loss function $L(\params, \hypers, t)$
   \State initialize $\vv_1 = \vzero$
   \For{$t=1$ {\bfseries to} $T$}
   \State $\vg_t = \gradparams$ \Comment{evaluate gradient}
   \State $\vv_{t+1} = \decay_t \vv_t - (1 - \decay_t) \vg_t$ \Comment{update velocity}  \label{step:update velocity}
   \State $\params_{t+1} = \params_t + \stepsize_t \vv_t$ \Comment{update position} \label{step:update position}
   \EndFor
   \State \textbf{output} trained parameters $\params_T$
\end{algorithmic}
\end{algorithm}
\end{minipage}
}
\vspace{0.3cm}
\begin{itemize}
\item Each gradient evaluation in SGD requires forward and backprop through model
\item Entire learning procedure looks like a 1000-layer deep net
\end{itemize}

}

\frame[plain]{\frametitle{Autograd: Automatic Differentiation} 
\begin{itemize} 
  \item \url{github.com/HIPS/autograd}
  %\item Simple ($\sim$ 300 lines of code)
  %\item Functional interface
  \item Works with (almost) arbitrary Python/Numpy code
  \item Can take gradients of gradients (of gradients...)
\end{itemize}}

\frame[plain]{\frametitle{Autograd Example}
\begin{columns}
\begin{column}{5.5cm}
\lstinputlisting{code/sinusoid-taylor.py} 
\end{column}
\begin{column}{5cm}
\includegraphics[width=6cm]{code/sinusoid_taylor.png}
\end{column}
\end{columns}}

\frame[plain]{\frametitle{Most Numpy functions implemented}
\vspace{0.5cm}
\newcommand{\us}{\textunderscore}
\centerline{
\begin{tabular}{llllllll}
 Complex             & Array         & Misc      & Linear   & Stats   \\
 \& Fourier          &               &           & Algebra  &          \\
\midrule
 imag                & atleast\us 1d & logsumexp & inv      & std      \\
 conjugate           & atleast\us 2d & where     & norm     & mean  \\
 angle               & atleast\us 3d & einsum    & det      & var   \\
 real\us if\us close & full          & sort      & eigh     & prod     \\
 real                & repeat        & partition & solve    & sum       \\
 fabs                & split         & clip      & trace    & cumsum  \\
 fft                 & concatenate   & outer     & diag     &        \\
 fftshift            & roll          & dot       & tril     &         \\
 fft2                & transpose     & tensordot & triu     &        \\
 ifftn               & reshape       & rot90     &          &        \\
 ifftshift           & squeeze       &           &          &        \\
 ifft2               & ravel         &           &          &        \\
 ifft                & expand\us dims&           &          &       
\end{tabular}}
}


\frame[plain]{\frametitle{Technical Challenge: Memory}
\begin{itemize}
\item Reverse-mode differentiation needs access to entire learning trajectory
\item i.e.\ $10^7$ parameters $\times$ $10^5$ training iterations
\item Only need access in reverse order...
\item Could we recompute the learning trajectory backwards by running reverse SGD?
\end{itemize}}

\frame[plain]{\frametitle{SGD with momentum is reversible}
Forward update rule:
\begin{align*}
\vx_{t+1} & \leftarrow \vx_t + \alpha \vv_t \\
\vv_{t+1} & \leftarrow \beta \vv_t - \nabla L\left(\vx_{t+1}\right)
\end{align*}
Reverse update rule:
\begin{align*}
\vv_{t} & \leftarrow \left(\vv_{t+1} + \nabla L\left(\vx_{t+1}\right)\right)/\beta \\
\vx_{t} & \leftarrow \vx_{t+1} - \alpha \vv_t
\end{align*}
}

\frame[plain]{\frametitle{Reverse-mode differentiation of SGD}
\hspace{-0.9cm}\begin{minipage}[t]{0.5\columnwidth}
\begin{algorithm}[H]
\tiny
   \caption*{Stochastic Gradient Descent}
   \label{alg:sgd}
\begin{algorithmic}[1]
   \State {\bfseries input:} initial $\params_1$, decays $\decays$, learning rates $\stepsizes$, loss function $L(\params, \hypers, t)$
   \State initialize $\vv_1 = \vzero$
   \For{$t=1$ {\bfseries to} $T$}
   \State $\vg_t = \gradparams$ \Comment{evaluate gradient}
   \State $\vv_{t+1} = \decay_t \vv_t - (1 - \decay_t) \vg_t$ \Comment{update velocity}  \label{step:update velocity}
   \State $\params_{t+1} = \params_t + \stepsize_t \vv_t$ \Comment{update position} \label{step:update position}
   \EndFor
   \State \textbf{output} trained parameters $\params_T$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\columnwidth}
\begin{algorithm}[H]
   \caption*{Reverse-Mode Gradient of SGD}
   \label{alg:reverse-sgd}
\begin{algorithmic}[1]
\tiny
   \State {\bfseries input:} $\params_T$, $\vv_T$, $\decays$, $\stepsizes$, train loss $L(\params, \hypers, t)$, loss $f(\params)$
   \State initialize $d\vv = \vzero$, $d\hypers = \vzero$, $d\stepsize_t = \vzero$, $d\decay = \vzero$
   \State initialize $d\params = \nabla_\params f(\params_T)$
   \For{$t=T$ {\bfseries counting down to} $1$}
   \State $d\stepsize_t = d\params\tra \vv_t$
   \State $\params_{t-1} = \params_t - \stepsize_t \vv_t$ \Comment{downdate position}
   %\vspace{-0.95\baselineskip}
   \State $\vg_t = \gradparams$ \Comment{evaluate gradient}
   %\hfill \scalebox{1.1}{\Bigg\}} \vspace{-\baselineskip} \begin{minipage}{2.5cm} reverse \\ SGD \\ ops \strut \end{minipage}
   \State $\vv_{t-1} = [\vv_t + (1 - \decay_t) \vg_t] / \decay_t$ \Comment{downdate velocity}
   \State $d\vv = d\vv + \stepsize_t d\params$
   \State $d\decay_t = d\vv\tra (\vv_t + \vg_t)$
   \State $d\params = d\params - (1 - \decay_t) d\vv \nabla_\params \gradparams$ \label{line:hvp1}
   \State $d\hypers = d\hypers - (1 - \decay_t) d\vv \nabla_\hypers \gradparams$ \label{line:hvp2}
   \State $d\vv = \decay_t d\vv$
   \EndFor
   \State \textbf{output} gradient of $f(\params_T)$ w.r.t $\params_1$, $\vv_1$, $\decays$, $\stepsizes$ and $\hypers$
\end{algorithmic}
\end{algorithm}
\end{minipage}

\vspace{0.5cm}

\begin{itemize}
\item Outputs gradients with respect to all hypers.
\item Reversing SGD avoids storing learning trajectory
\end{itemize}
}


\frame[plain]{\frametitle{Naive reversal \phantom{... Fails!}}
\includegraphics[width=0.9\columnwidth]{../experiments/Jan_25_Figure_1/4_naive_reverse/learning_curve_forward.pdf}}

\frame[plain]{\frametitle{Naive reversal ... Fails!}
\includegraphics[width=0.9\columnwidth]{../experiments/Jan_25_Figure_1/4_naive_reverse/learning_curve_reverse.pdf}}

\frame[plain]{\frametitle{A closer look at reverse SGD}
Forward update rule:
\begin{align*}
\vx_{t+1} & \leftarrow \vx_t + \alpha \vv_t \\
\vv_{t+1} & \leftarrow \beta \vv_t - \nabla L\left(\vx_{t+1}\right)
\end{align*}
%\begin{itemize}
%\item 
Destroys $\log_2 \beta$ bits per parameter per iteration
%\end{itemize}

\vspace{1cm}

Reverse update rule:
\begin{align*}
\vv_{t} & \leftarrow \left(\vv_{t+1} + \nabla L\left(\vx_{t+1}\right)\right)/\beta \\
\vx_{t} & \leftarrow \vx_{t+1} - \alpha \vv_t
\end{align*}
%\pause
%\begin{itemize}
%\item 
Needs $\log_2 \beta$ bits per parameter per iteration 
%\end{itemize}
}

\newcommand{\trail}[2]{\includegraphics<#2>[width=6cm]{talkfigs/trails_0/iter_#1.pdf}}%
\newcommand{\dist}[2]{\includegraphics<#2>[width=6cm]{talkfigs/dists/dists_#1.pdf}}%

\frame[plain]{\frametitle{Optimization removes entropy}
\begin{columns}
\hspace{-1cm}\begin{column}{6cm}
\begin{itemize} 
  \item Optimization starts from random init, and move towards modes
  \item Must forget where we started
\end{itemize}
\end{column}
\begin{column}{5cm}
\trail{0}{1}%
\trail{1}{2}%
\trail{2}{3}%
\trail{3}{4}%
\trail{4}{5}%
\trail{5}{6}%
\trail{6}{7}%
\trail{7}{8}%
\trail{8}{9}%
\trail{9}{10}%
\trail{10}{11}%
\dist{0}{12}%
\dist{1}{13}%
\dist{2}{14}%
\dist{3}{15}%
\dist{4}{16}%
\dist{5}{17}%
\dist{6}{18}%
\dist{7}{19}%
\dist{8}{20}%
\dist{9}{21}%
\dist{10}{22}%
\end{column}
\end{columns}}


\lstset{basicstyle=\small\ttfamily\bfseries}

\frame[plain]{\frametitle{How to store the lost bits?}
\begin{itemize} 
\item Switch to fixed-precision for exact addition
\item Express $\beta$ as a rational number
\item push/pop remainders from an information buffer
\end{itemize}
%\item Information buffer can use any LIFO coding method
\lstinputlisting{code/infobuffer.py}
\begin{itemize} 
\item 200X memory savings when $\beta = 0.9$
\item Now we have scalable gradients of hypers, only twice as slow as original!
\end{itemize}
}

\frame[plain]{\frametitle{}

\Huge
\centerline{
Part 2: A Garden of Delights}
}


\frame[plain]{\frametitle{Learning rate gradients}
\begin{columns}
\begin{column}{6cm}
\begin{align*}
\frac{\partial Loss \left( D_{val}, \params_{init}, \learnrate, \decay, D_{train}, \textnormal{reg} \right)}{\partial \learnrate}
\end{align*} 
\end{column}
\begin{column}{4cm}
\includegraphics[width=0.6\columnwidth, clip, trim=9cm 3cm 0.8cm 0.5cm]{../experiments/Feb_3_training_schedules/3_adam_50/schedules_small.pdf}
\end{column}
\end{columns}

\includegraphics[width=\columnwidth]{../experiments/Feb_3_training_schedules/5_initial_gradient/schedules_small.pdf}
}

\frame[plain]{\frametitle{Optimized learning rates}
\begin{itemize}
\item Used SGD to optimize SGD
\item 4-layer NN on MNIST
\item Top layer learns early on; slowdown at end
\end{itemize}
\includegraphics[width=\columnwidth]{../experiments/Feb_3_training_schedules/3_adam_50/schedules_small.pdf}}

\frame[plain]{\frametitle{Optimizing initialization scales} 
\begin{center}
\begin{align*}
\frac{\partial Loss \left( D_{val}, \params_{init}, \learnrate, \decay, D_{train}, \textnormal{reg} \right)}{\partial \params_{init}}
\end{align*}
\begin{tabular}{cc}
 Biases & Weights \\
\hspace{-1em}\includegraphics[width=0.5\columnwidth, height=0.5\columnwidth]{../experiments/Feb_3_training_schedules/3_adam_50/init_bias_learning_curve.pdf} &
\hspace{-1em}\includegraphics[width=0.5\columnwidth, height=0.5\columnwidth]{../experiments/Feb_3_training_schedules/3_adam_50/init_weight_learning_curve.pdf}
\end{tabular}
\end{center}}

\frame[plain]{\frametitle{Optimizing regularization}
\center
\begin{align*}
\frac{\partial Loss \left( D_{val}, \params_{init}, \learnrate, \decay, D_{train}, \textnormal{reg} \right)}{\partial \textnormal{reg}}
\end{align*}
Optimized $L_2$ hypers for each weight in logistic regression:
\vspace{0.5in}
\includegraphics[width=\columnwidth]{../experiments/Jan_21_nn_ard/2/penalties.pdf}
}

\frame[plain]{\frametitle{Optimizing architecture}
\begin{center}
\begin{itemize} 
  \item Architecture = tying weights or setting to them zero
  \item i.e. convnets, recurrent nets, multi-task  
  \item Trying be enforced by L2 regularization
  \item L2 regularization is differentiable
\end{itemize}
\vspace{1cm}
\begin{columns}
\begin{column}{5.5cm}
\begin{tabular}{cc}
\hspace{-3mm}\rotatebox{90}{\tiny{ \qquad Rotated \qquad \quad Original}} & 
\hspace{-3mm}\includegraphics[width=0.85\columnwidth]{../experiments/Feb_4_augmented_omniglot/2_rotated_90/all_alphabets.png}
\end{tabular}
\end{column}
\begin{column}{5cm}
\includegraphics[width=\columnwidth]{talkfigs/shared-net.png}
\end{column}
\end{columns}
\end{center}}

\newcommand{\omniimagea}[2]{\parbox{4em}{\includegraphics[width=1.4cm]{../experiments/Feb_4_augmented_omniglot/2_rotated_90/minifigs/learned_corr_#1_#2.pdf}}}%
\newcommand{\omniimageb}[1]{\omniimagea{#1}{0} & \omniimagea{#1}{1} & \omniimagea{#1}{2}}%

\frame[plain]{\frametitle{Optimizing architecture}
\begin{center}
Matrices enforce weight sharing between tasks
\vspace{0.5cm}

\begin{tabular}{c@{\hskip 0.9em}ccc@{\hskip 0.9em}c@{\hskip 0.9em}c}%
\renewcommand{\tabcolsep}{1pt}
& Input   & Middle  & Output & Train & Test\\
& weights & weights & weights & error & error \\
\parbox{3.7em}{Separate networks} & \omniimageb{no_sharing}      & 0.61 & 1.34\\ \hline
\parbox{3.7em}{Tied weights}      & \omniimageb{full_sharing}    & 0.90 & 1.25\\ \hline
\parbox{3.7em}{Learned sharing}   & \omniimageb{learned_sharing} & 0.60 & \bf 1.13
\end{tabular}
\end{center}}

\frame[plain]{\frametitle{Optimizing training data}
\center
\begin{align*}
\frac{\partial Loss \left( D_{val}, \params_{init}, \learnrate, \decay, D_{train}, \textnormal{reg} \right)}{\partial D_{train}}
\end{align*}
\begin{itemize}
\item Training set of size 10 with fixed labels on MNIST
\item Started from blank images
\end{itemize}
%\vspace{0.5in}
\includegraphics[width=\columnwidth]{../experiments/Jan_19_optimize_data/9_color_bar/fake_data.pdf}
}

\frame[plain]{\frametitle{Limitations: Chaotic learning dynamics}
\includegraphics[width=0.9\columnwidth]{../experiments/Jan_14_learning_rate_wiggliness/3/chaos.pdf}
\vspace{-1.6em}
\center Learning rate}



%% \frame[plain]{\frametitle{Other possible applications}} % DM
\frame[plain]{\frametitle{Summary}
\begin{itemize}
\item Can compute gradients of learning procedures
\item Reversing learning saves memory
\item Can optimize thousands of hyperparameters
%\item There is a deep connection between learning and entropy
\end{itemize}

\pause
\centerline{Thanks!}
}

\end{document}

\frame[plain]{\frametitle{Entropy and Optimization are Duals}
\centerline{\includegraphics[width=0.9\columnwidth]{talkfigs/dists}}}


\frame[plain]{\frametitle{Reverse-mode Differentiation}
\begin{align*}
\onslide<1->{\pderiv{}{x} L\paren{F\paren{G\paren{H\paren{x}}}}
             & = \pderiv{L}{F}\pderiv{F}{G}\pderiv{G}{H}\pderiv{H}{x} \\}
\onslide<2->{& = \pderiv{L}{F}\paren{\pderiv{F}{G}\paren{\pderiv{G}{H}\pderiv{H}{x}}} %\quad &\text{Forward mode}
\\
             & = \paren{\paren{\pderiv{L}{F}\pderiv{F}{G}}\pderiv{G}{H}}\pderiv{H}{x} %\quad &\text{Reverse mode}
             }
\end{align*}
\onslide<3>{
\begin{tikzpicture}[thick, scale=0.4]
  \draw (1,3.2) rectangle (6, 3.8);
  \draw (8,1) rectangle (13, 6);
  \draw (15,1) rectangle (20, 6);
  \draw (22,1) rectangle (27, 6);
  \node at (7, 3.5) {$\times$};
  \node at (14, 3.5) {$\times$};
  \node at (21, 3.5) {$\times$};
\end{tikzpicture}}
}


