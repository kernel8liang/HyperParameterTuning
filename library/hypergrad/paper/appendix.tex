\documentclass{article}
\usepackage{times}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{arxiv} 
%\usepackage[accepted]{icml2015stylefiles/icml2015}


\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\newcommand{\primal}{elementary}
\newcommand{\Primal}{Elementary}
\newcommand{\numhypers}{N}
\newcommand{\hypers}{{\boldsymbol{\theta}}}
\newcommand{\params}{\vw}
\newcommand{\numsteps}{T}
\newcommand{\decay}{\gamma}
\newcommand{\decays}{{\boldsymbol{\decay}}}
\newcommand{\stepsize}{\alpha}
\newcommand{\stepsizes}{{\boldsymbol{\stepsize}}}
\newcommand{\gradparams}{\nabla_\params L(\params_t, \hypers, t)}

\newcommand\ourtitle{Gradient-based Hyperparameter Optimization through Reversible Learning: Appendix}
\icmltitlerunning{\ourtitle} % A short form for the running title.


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document} 

\twocolumn[
\icmltitle{\ourtitle}
\icmlauthor{Dougal Maclaurin$^\dagger$}{maclaurin@physics.harvard.edu}
\icmladdress{Harvard University, Cambridge, Massachusetts}
\icmlauthor{David Duvenaud$^\dagger$}{dduvenaud@seas.harvard.edu}
\icmladdress{Harvard University, Cambridge, Massachusetts}
\icmlauthor{Ryan P. Adams}{rpa@seas.harvard.edu}
\icmladdress{Harvard University, Cambridge, Massachusetts}
]

\vspace*{0cm}

\section*{Forward vs. reverse-mode differentiation}
\label{sec:appendix}
By the chain rule, the gradient of a set of nested functions is given by the product of the individual derivatives of each function:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \pderiv{f_4}{f_3} \cdot \pderiv{f_3}{f_2} \cdot \pderiv{f_2}{f_1} \cdot \pderiv{f_1}{x}
\end{align*}
If each function has multivariate inputs and outputs, the gradients are
Jacobian matrices.

Forward and reverse mode differentiation differ
only by the order in which they evaluate this product.
%
Forward-mode differentiation works by multiplying gradients in the same order as
the functions are evaluated:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \pderiv{f_4}{f_3} \cdot \left( \pderiv{f_3}{f_2} \cdot \left( \pderiv{f_2}{f_1} \cdot \pderiv{f_1}{x} \right) \right)
\end{align*}
%
Reverse-mode multiplies the gradients in the opposite order, starting from the
final result:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \left(  \left(  \pderiv{f_4}{f_3} \cdot \pderiv{f_3}{f_2} \right) \cdot \pderiv{f_2}{f_1} \right) \cdot \pderiv{f_1}{x} 
\end{align*}
%
In an optimization setting, the final result of the nested functions, $f_4$, is
a scalar, while the input $x$ and intermediate values, $f_1 - f_3$, can be
vectors. In this scenario the advantage of reverse-mode
differentiation is very clear. Let's imagine that the dimensionality of all the
intermediate vectors is $D$. In reverse mode, we start from the (scalar) output,
and multiply by the next $D \times D$ Jacobian at each step. The value we
accumulate is just a $D$-dimensional vector. In forward mode, however, we must
accumulate an entire $D \times D$ matrix at each step. But do we have still
have to compute and instantiate the $D \times D$ Jacobian matrices themselves
either way?  In general, yes. But in the (common) case that the vector-to-vector
functions are either elementwise operations or (reshaped) matrix multiplications, the
Jacobian matrices can actually be very sparse, and multiplication by the
Jacobian can be performed efficiently without instantiation~\cite{pearlmutter2008reverse}.

The main drawback of reverse-mode differentiation is that intermediate values
must be maintained in memory during the forward pass. In sections
2.1 and 2.3, we show how
to drastically reduce the memory requirements of reverse-mode differentiation
when differentiating through the entire learning procedure.

\bibliography{references.bib}
\bibliographystyle{icml2015stylefiles/icml2015}


\end{document} 
