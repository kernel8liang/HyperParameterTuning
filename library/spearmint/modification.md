
###Working summary 1
####Integrate spearmint with block to run MLP

#####Preparation:
###### 1.  Install spearmint and block independly.
https://github.com/JasperSnoek/spearmint
	https://github.com/mila-udem/blocks
	>block need fuel to import data. so fuel is also needed.
	
https://github.com/mila-udem/fuel
##### Integration
######1. Create a hyperopt_experiment folder in computer. eg. Here my path of hyperopt_experiment is `/home/yumeng/Documents/hyperopt_experiment`
Corresponding path in file Spearmint/Spearmint/main.py should be modified as well.
###### 2. hyperopt_experiment contains two files: 
| hyperyaml.yaml      |    spear_config.pb | 
| :-------- | --------:| 
| This file contain the actual hyper parameter value in project. With format | This file contain the module to run when want to conduct the real deep learning progress.  hyper-parameter will also be passed to the class. Variable format will also be contained in the file |  
-  Example for hyperyaml.yaml
```
learning-rate: !hyperopt lr FLOAT 0.01 0.1,
```
 - Example for spear_config.pb 
```
language: PYTHON 
name: "blocks_runner.blocks_gen_yaml_and_run"

variable {
        name: "lr"
        type: FLOAT
        size: 1
        min:  0.01
        max:  0.1
        }
```
>variable can be one or more, and all variables in hyperyaml.yaml should be corresponding to the variables in spear_config.pb file. 

###### 3. To run spearmint, run the `main.py` file at spearmint/spearmint.
> Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms

The file will read in the spear_config.pb file, all the variables contained in the spear_config are read as the format to generate new hyper-parameter. Taking the previous file as example, new generated parameter will be passed to blocks_runner.blocks_gen_yaml_and_run to run the process in the class.

###### 4. Add in all the MLP training and testing codes(using framework block) in file blocks_runner.blocks_gen_yaml_and_run. 
param passed to the function is a dictionary of all hyper-parameters.
Example: currently the hyper-parameter passed is:
`lr : 0.01 `and `nep : 50` the param will be the dict containing the two. 
This module should return the loss value after the deep learning progress so that spearmint will generate the new pair of hyper-parameters.

During the progress, continuous loss value for each epochs progress will be written into a file named `temp_error_file` in experiment folder
File regarding this part is blocks/extensions/_init_.py
modified do function 
```
def do(self, which_callback, *args):
        log = self.main_loop.log
        print_status = True
        print()
        if print_status:
            print("Training status:")
            #self._print_attributes(log.status)
            print("Log records from the iteration {}:".format(
                log.status['iterations_done']))
            self._print_attributes(log.current_row)
            with open("/home/yumeng/Documents/hyperopt_experiment/temp_error_file", 'w') as fout:
                for attr, value in sorted(log.current_row.items(), key=first):
                    if not attr.startswith("_"):
                        fout.write(str(value)+" ")

        print()
```

###Working summary 2
####Using current Yaml to re-run in larger dataset
Either using neon with spearmint or using blocks with spearmint will generate the yaml files every time hyper-parameters changed.

After we running the training and testing progress, yamls will be generated, here we will use all the yaml files for a larger amount of data to test if the model generated by a smaller set of data will have a good predict result compared by the model generated by a larger set of data.

#####Progress
To read in the current yaml data rather than generate new hyper-parameters, I define one `mode` parameter in main.py file.

When mode == `generate`,  the spearmint generate new hyper-parameter function will be called to generate the next best candidate hyper-parameter.
Otherwise, mode will be equal to `hyper_to_yaml`, and the current yaml files will be read in to get the hyper-parameter generated from the smaller set of data. After getting the value of all hyper_parameters, the process will be much similar to the previous one, either the module written using neon framework or the module written using blocks will be called to run the training and testing progress.
